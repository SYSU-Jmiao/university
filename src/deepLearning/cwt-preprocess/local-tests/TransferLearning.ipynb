{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os,random\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"KERAS_FLAGS\"]  = \"device=gpu%d\"%(0)\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import keras.models as models\n",
    "from keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.regularizers import *\n",
    "from keras.optimizers import adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cPickle, random, sys, keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: [u'Classes', u'LabelClass', u'LabelSnr', u'Snrs', u'X_samples', u'Y_samples']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = '../data_cwt.hdf5'\n",
    "f = h5py.File(filename, 'r')\n",
    "\n",
    "# List all groups\n",
    "print(\"Keys: %s\" % f.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = f['X_samples']\n",
    "Y = f['Y_samples']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2016)\n",
    "n_examples = X.shape[0]\n",
    "n_train = n_examples * 0.5\n",
    "train_idx = np.random.choice(range(0,int(n_examples)), size=int(n_train), replace=False)\n",
    "test_idx = list(set(range(0,n_examples))-set(train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 224 3\n"
     ]
    }
   ],
   "source": [
    "img_width ,img_height,img_dim =  X[0].shape[0],X[0].shape[1],X[0].shape[2]\n",
    "print img_width ,img_height ,img_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80117760/80134624 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, img_dim))\n",
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "for layer in model.layers[:5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = 11\n",
    "#Adding custom Layers \n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(NUMBER_OF_CLASSES, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoni/repos/university/ENV/lib/python2.7/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model \n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(input = model.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model \n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = keras.optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "def samples_generator(x_list,y_list,index_list,batch_size):\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
